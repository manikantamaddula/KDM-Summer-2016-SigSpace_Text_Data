# iHear
There are approximately 70 million deaf people in the world. For many, hearing aids, sign language, cochlear implants and subtitles are useful. Some develop Lip reading skill. However, access is limited to above mentioned technology to many people. So, a mobile application which can recognize speech from user and display summary text about what user is speaking would be very helpful and cost effective. There are several speech to text recognition API's available. These can be used to convert audio to text data. There has been a lot of development in natural language processing and semantic applications. Converted text can be summarized by using machine learning algorithms. But since computing power that is available in mobile is limited, light weight machine learning model can be used in client side to improve the performance and recognition time. Self-Organizing maps and sigspace feature modelling can be used to achieve light weight machine learning. Sample data is used for training (or learning) and model building. Speech to text API's are used to generate text data. From the converted text data, word embedding algorithms can be used to extract keywords. Here Ontology models are generated based on training data. To work with large models or large streaming data, apache spark can be used. Apache Spark is an open source big data processing framework which provides high performance and sophisticated analytics. Dynamic recognition can be used to generate topic data or summary data.

The overall objective is to build an application which can recognize speech and generate topic data or summary data in text format.
